{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_std = scaler.transform(Xtr)\n",
    "Xts_std = scaler.transform(Xts)\n",
    "\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "Xtr_vec = vec.fit_transform(Xtr_str)\n",
    "Xts_vec = vec.transform(Xts_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "features = ['Parks_Nearby', 'Grocery_Stores_Nearby', 'Schools_Nearby', 'Public_Transit_Nearby']\n",
    "target = ['Walkability_Score']\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "random_state = 23\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "model = LinearRegression().fit(Xtr, ytr)\n",
    "yts_hat = model.predict(Xts)\n",
    "rsq = r2_score(yts, yts_hat)\n",
    "mse = mean_squared_error(yts, yts_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single split - random shuffle\n",
    "random_state = 8\n",
    "Xtr_one_shuf, Xts_one_shuf, ytr_one_shuf, yts_one_shuf = train_test_split(X, y, test_size=1/5, random_state=random_state)\n",
    "model = LinearRegression()\n",
    "model.fit(Xtr_one_shuf, ytr_one_shuf)\n",
    "yts_one_shuf_pred = model.predict(Xts_one_shuf)\n",
    "r2_one_shuf = r2_score(yts_one_shuf, yts_one_shuf_pred)\n",
    "# Single split - sorted data, no shuffle\n",
    "Xtr_one_order, Xts_one_order, ytr_one_order, yts_one_order = train_test_split(X, y, test_size=1/5, shuffle=False)\n",
    "model = LinearRegression()\n",
    "model.fit(Xtr_one_order, ytr_one_order)\n",
    "yts_one_order_pred = model.predict(Xts_one_order)\n",
    "r2_one_order = r2_score(yts_one_order, yts_one_order_pred)\n",
    "# Multiple splits - random shuffle\n",
    "n_fold = 5\n",
    "r2_kf_shuffle = np.zeros(shape=(n_fold,))\n",
    "kf = KFold(n_splits=n_fold, shuffle=True, random_state=random_state)       \n",
    "for i, (idx_tr, idx_ts) in enumerate(kf.split(X)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[idx_tr], y[idx_tr])\n",
    "    y_pred_kfold = model.predict(X[idx_ts])\n",
    "    r2_kf_shuffle[i] = r2_score(y[idx_ts], y_pred_kfold)\n",
    "r2_kf_shuffle_mean = np.mean(r2_kf_shuffle)\n",
    "# Multiple splits - time series\n",
    "n_fold = 5\n",
    "r2_ts = np.zeros(shape=(n_fold,))\n",
    "ts = TimeSeriesSplit(n_splits=n_fold)\n",
    "for i, (idx_tr, idx_ts) in enumerate(ts.split(X)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[idx_tr], y[idx_tr])\n",
    "    y_pred = model.predict(X[idx_ts])\n",
    "    r2_ts[i] = r2_score(y[idx_ts], y_pred)\n",
    "r2_ts_mean = np.mean(r2_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\n",
    "y_names = ['price']\n",
    "random_state = 13\n",
    "Xtr_df, Xts_df = train_test_split(df[x_names], test_size=0.3, random_state=random_state, shuffle=True)\n",
    "ytr_df, yts_df = train_test_split(df[y_names], test_size=0.3, random_state=random_state, shuffle=True)\n",
    "Xtr, Xts, ytr, yts = np.array(Xtr_df), np.array(Xts_df), np.array(ytr_df), np.array(yts_df)\n",
    "alpha_list = np.array([0, 10, 20, 50, 100, 200, 500])\n",
    "nfold = 5\n",
    "mse_val = np.zeros((len(alpha_list), nfold))\n",
    "# k-ford\n",
    "kf = KFold(n_splits=nfold, shuffle=False)\n",
    "# For each fold, standardize the data\n",
    "for ifold, (idx_tr, idx_val) in enumerate(kf.split(Xtr)):\n",
    "    X_train_fold, X_val_fold = Xtr[idx_tr], Xtr[idx_val]\n",
    "    y_train_fold, y_val_fold = ytr[idx_tr], ytr[idx_val]\n",
    "    scaler = StandardScaler().fit(X_train_fold)\n",
    "    X_train_fold_std = scaler.transform(X_train_fold)\n",
    "    X_val_fold_std = scaler.transform(X_val_fold)\n",
    "    # For each alpha in the list, fit a Ridge regression model on the standardized data\n",
    "    for i, alpha in enumerate(alpha_list):\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_train_fold_std, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold_std)\n",
    "        # update the appropriate entry in mse_val\n",
    "        mse_val[i, ifold] = mean_squared_error(y_val_fold, y_pred)\n",
    "mse_mean = np.mean(mse_val, axis=1)\n",
    "alpha_min_mse = alpha_list[np.argmin(mse_mean)]\n",
    "# entire training set\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_std = scaler.transform(Xtr)\n",
    "Xts_std = scaler.transform(Xts)\n",
    "model = Ridge(alpha=alpha_min_mse)\n",
    "model.fit(Xtr_std, ytr)\n",
    "y_pred = model.predict(Xts_std)\n",
    "mse_ridge = mean_squared_error(yts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize data in each fold.\n",
    "random_state = 14\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "med_values = np.nanmedian(Xtr, axis=0)\n",
    "Xtr_filled = np.nan_to_num(Xtr, nan=med_values)\n",
    "Xts_filled = np.nan_to_num(Xts, nan=med_values)\n",
    "C_test = np.logspace(-1,3,10)\n",
    "nfold = 3\n",
    "acc_val = np.zeros((len(C_test), nfold))\n",
    "for iC, C in enumerate(C_test): \n",
    "    kf = KFold(n_splits=nfold, shuffle=False)\n",
    "    for ifold, (Itr, Ival) in enumerate(kf.split(Xtr_filled)):\n",
    "        Xtr_fold, Xval_fold = Xtr_filled[Itr], Xtr_filled[Ival]\n",
    "        ytr_fold, yval_fold = ytr[Itr], ytr[Ival]     \n",
    "        scaler = StandardScaler().fit(Xtr_fold)\n",
    "        Xtr_std = scaler.transform(Xtr_fold)\n",
    "        Xvl_std = scaler.transform(Xval_fold)\n",
    "        clf = LogisticRegression(random_state = random_state, solver = 'liblinear', penalty='l1', C = C)\n",
    "        clf.fit(Xtr_std, ytr_fold)\n",
    "        yhat = clf.predict(Xvl_std)\n",
    "        acc_val[iC, ifold] = accuracy_score(yval_fold, yhat)\n",
    "acc_mean = np.mean(acc_val, axis=1)\n",
    "C_best = C_test[np.argmax(acc_mean)]\n",
    "# entire training set\n",
    "scaler = StandardScaler().fit(Xtr_filled)\n",
    "Xtr_std = scaler.transform(Xtr_filled)\n",
    "Xts_std = scaler.transform(Xts_filled)\n",
    "clf_best = LogisticRegression(random_state = random_state, solver = 'liblinear', penalty='l1', C = C_best)\n",
    "clf_best.fit(Xtr_std, ytr)\n",
    "y_hat = clf_best.predict(Xts_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_str, Xts_str, ytr, yts = train_test_split(df['statement'].values, df['label_binary'].values, shuffle=True, random_state=0, test_size=0.25)\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "Xtr_vec = vec.fit_transform(Xtr_str)\n",
    "Xts_vec = vec.transform(Xts_str)\n",
    "C_test = np.logspace(-3, 3, num=20)\n",
    "nfold = 5\n",
    "acc_val = np.zeros((len(C_test), nfold))\n",
    "kf = KFold(n_splits=nfold)\n",
    "for ifold, (Itr, Ival) in enumerate(kf.split(Xtr_vec)):\n",
    "    for iC, C in enumerate(C_test):\n",
    "        clf = LogisticRegression(random_state = 0, penalty='l1', solver='liblinear', C = C)\n",
    "        clf.fit(Xtr_vec[Itr], ytr[Itr])\n",
    "        yhat = clf.predict(Xtr_vec[Ival])\n",
    "        acc_val[iC, ifold] = accuracy_score(ytr[Ival], yhat)\n",
    "acc_mean = np.mean(acc_val, axis=1)\n",
    "C_best = C_test[np.argmax(acc_mean)]\n",
    "model_best = LogisticRegression(penalty='l1', C=C_best, random_state=0, solver='liblinear')\n",
    "model_best.fit(Xtr_vec, ytr)\n",
    "y_pred_best = model_best.predict(Xts_vec)\n",
    "acc_best = accuracy_score(yts, y_pred_best)\n",
    "count_best = np.count_nonzero(model_best.coef_)\n",
    "acc_std = acc_val.std(axis=1)\n",
    "acc_one_se = acc_mean - acc_std\n",
    "C_one_se = C_test[np.argmax(acc_mean >= np.max(acc_one_se))]\n",
    "model_one_se = LogisticRegression(penalty='l1', C=C_one_se, random_state=0, solver='liblinear')\n",
    "model_one_se.fit(Xtr_vec, ytr)\n",
    "y_pred_one_se = model_one_se.predict(Xts_vec)\n",
    "acc_one_se = accuracy_score(yts, y_pred_one_se)\n",
    "count_one_se = np.count_nonzero(model_one_se.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold CV with Fourier basis expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You decide to use a linear regression model with Fourier basis transformation of the `hourofweek` feature:\n",
    "\n",
    "$$\\hat{y}=w_0 + w_1 x +  \\sum_{t \\in \\text{tlist}} w_{t,c} \\cos(2\\pi x/t)+w_{t,s} \\sin(2\\pi x/t)$$\n",
    "\n",
    "where each sine and cosine pair represents the periodic behavior over a particular time interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if `tlist = [0.5, 1]`, then your model would be:\n",
    "\n",
    "$$\\hat{y}=w_0 + w_1 x + w_{0.5,c} \\cos(2\\pi x/0.5)+w_{0.5,s} \\sin(2\\pi x/0.5) + w_{1,c} \\cos(2\\pi x/1)+w_{1,s} \\sin(2\\pi x/1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to fit a K-fold CV! In your CV, you will fit and evaluate a `LinearRegression` model (using the `sklearn` implementation) on an increasing number of columns of the data, as described above - \n",
    "\n",
    "* in the first iteration of your K-fold CV, you will evaluate the regression for `tlist_eval = []`. \n",
    "* In the second iteration of your K-fold CV, you will evaluate the regression for `tlist_eval = [1]`.\n",
    "* In the third iteration, you will evaluate the regression for `tlist_eval = [1, 2]`\n",
    "\n",
    "and so on, until, in the final iteration, you will evaluate the regression for *all* the values in `tlist`.\n",
    "\n",
    "(Of course, you won't re-compute the Fourier basis transformation inside the loop - you'll just select the appropriate rows and columns from `Xtr_trans` in each iteration.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have prepared a \"ones column\" in the data, you will pass `fit_intercept=False` as an argument to the `LinearRegression`, so that it won't also fit another \"intercept\" term (in addition to the coefficient for the \"ones column\".)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "df_tr, df_ts = train_test_split(df, train_size=10000, shuffle=False, random_state=42)\n",
    "features = ['hour', 'month', 'dayofweek', 'hourofweek']\n",
    "target = 'ridership'\n",
    "model = LinearRegression()\n",
    "model.fit(df_tr[features], df_tr[target])\n",
    "y_pred = model.predict(df_ts[features])\n",
    "r2_lr = r2_score(df_ts[target], y_pred)\n",
    "# K-fold CV with fourier basis expansion\n",
    "tlist = np.arange(1, 51)\n",
    "Xtr = df_tr['hourofweek'].values\n",
    "Xts = df_ts['hourofweek'].values\n",
    "ytr = df_tr['ridership'].values\n",
    "yts = df_ts['ridership'].values\n",
    "Xtr_trans = np.column_stack((np.ones_like(Xtr), Xtr))\n",
    "Xts_trans = np.column_stack((np.ones_like(Xts), Xts))\n",
    "for t in tlist:\n",
    "    cos_tr = np.cos(2 * np.pi * Xtr / t)\n",
    "    sin_tr = np.sin(2 * np.pi * Xtr / t)\n",
    "    cos_ts = np.cos(2 * np.pi * Xts / t)\n",
    "    sin_ts = np.sin(2 * np.pi * Xts / t)\n",
    "    Xtr_trans = np.column_stack((Xtr_trans, cos_tr, sin_tr))\n",
    "    Xts_trans = np.column_stack((Xts_trans, cos_ts, sin_ts))\n",
    "Xtr_trans = np.round(Xtr_trans, 10)\n",
    "Xts_trans = np.round(Xts_trans, 10)\n",
    "\n",
    "nfold = 5\n",
    "r2_val = np.zeros((len(tlist) + 1, nfold))\n",
    "kf = KFold(n_splits=nfold, shuffle=False)\n",
    "for i, t in enumerate(range(len(tlist) + 1)):\n",
    "    # The first model uses only the intercept and original X (2 columns)\n",
    "    # Subsequent models add Fourier features in pairs (cos and sin)\n",
    "    num_columns = 2 + 2 * i \n",
    "    X_subset = Xtr_trans[:, :num_columns]\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_subset)):\n",
    "            X_train, X_val = X_subset[train_idx], X_subset[val_idx]\n",
    "            y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "            model = LinearRegression(fit_intercept=False)\n",
    "            model.fit(X_train, y_train)           \n",
    "            y_pred = model.predict(X_val)\n",
    "            r2_val[i, fold] = r2_score(y_val, y_pred)\n",
    "\n",
    "r2_mean = r2_val.mean(axis=1)\n",
    "r2_se = np.std(r2_val, axis=1, ddof=1) / np.sqrt(nfold)\n",
    "\n",
    "idx_max = np.argmax(r2_mean)\n",
    "# Compute the threshold for the one-SE rule\n",
    "one_se = r2_mean[idx_max] - r2_se[idx_max]\n",
    "# Find the simplest model (smallest number of Fourier features) whose R² is within one SE of the best model\n",
    "tlist_opt = []\n",
    "for i in range(len(r2_mean)):\n",
    "    if r2_mean[i] >= one_se:\n",
    "        tlist_opt = tlist[:i]  # Select the first i values of tlist\n",
    "        break\n",
    "# Train a model on the entire training set for this `tlist_opt`, then evaluate its performance on the test set. \n",
    "# Save the test R2 score in r2_one_se.\n",
    "num_columns = 2 + 2 * len(tlist_opt)\n",
    "Xtr_opt = Xtr_trans[:, :num_columns]\n",
    "Xts_opt = Xts_trans[:, :num_columns]\n",
    "model_opt = LinearRegression(fit_intercept=False)\n",
    "model_opt.fit(Xtr_opt, ytr)\n",
    "y_pred_opt = model_opt.predict(Xts_opt)\n",
    "r2_one_se = r2_score(yts, y_pred_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
