{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_std = scaler.transform(Xtr)\n",
    "Xts_std = scaler.transform(Xts)\n",
    "\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "Xtr_vec = vec.fit_transform(Xtr_str)\n",
    "Xts_vec = vec.transform(Xts_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "features = ['Parks_Nearby', 'Grocery_Stores_Nearby', 'Schools_Nearby', 'Public_Transit_Nearby']\n",
    "target = ['Walkability_Score']\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "random_state = 23\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "model = LinearRegression().fit(Xtr, ytr)\n",
    "yts_hat = model.predict(Xts)\n",
    "rsq = r2_score(yts, yts_hat)\n",
    "mse = mean_squared_error(yts, yts_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single split - random shuffle\n",
    "random_state = 8\n",
    "Xtr_one_shuf, Xts_one_shuf, ytr_one_shuf, yts_one_shuf = train_test_split(X, y, test_size=1/5, random_state=random_state)\n",
    "model = LinearRegression()\n",
    "model.fit(Xtr_one_shuf, ytr_one_shuf)\n",
    "yts_one_shuf_pred = model.predict(Xts_one_shuf)\n",
    "r2_one_shuf = r2_score(yts_one_shuf, yts_one_shuf_pred)\n",
    "# Single split - sorted data, no shuffle\n",
    "Xtr_one_order, Xts_one_order, ytr_one_order, yts_one_order = train_test_split(X, y, test_size=1/5, shuffle=False)\n",
    "model = LinearRegression()\n",
    "model.fit(Xtr_one_order, ytr_one_order)\n",
    "yts_one_order_pred = model.predict(Xts_one_order)\n",
    "r2_one_order = r2_score(yts_one_order, yts_one_order_pred)\n",
    "# Multiple splits - random shuffle\n",
    "n_fold = 5\n",
    "r2_kf_shuffle = np.zeros(shape=(n_fold,))\n",
    "kf = KFold(n_splits=n_fold, shuffle=True, random_state=random_state)       \n",
    "for i, (idx_tr, idx_ts) in enumerate(kf.split(X)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[idx_tr], y[idx_tr])\n",
    "    y_pred_kfold = model.predict(X[idx_ts])\n",
    "    r2_kf_shuffle[i] = r2_score(y[idx_ts], y_pred_kfold)\n",
    "r2_kf_shuffle_mean = np.mean(r2_kf_shuffle)\n",
    "# Multiple splits - time series\n",
    "n_fold = 5\n",
    "r2_ts = np.zeros(shape=(n_fold,))\n",
    "ts = TimeSeriesSplit(n_splits=n_fold)\n",
    "for i, (idx_tr, idx_ts) in enumerate(ts.split(X)):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X[idx_tr], y[idx_tr])\n",
    "    y_pred = model.predict(X[idx_ts])\n",
    "    r2_ts[i] = r2_score(y[idx_ts], y_pred)\n",
    "r2_ts_mean = np.mean(r2_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression K-fold\n",
    "The outer loop can be used to divide the data into training and validation, but then weâ€™ll also need an inner loop to train and validate each model for this particular fold.\n",
    "\n",
    "In this case, suppose we want to evaluate polynomial models with different model orders from\n",
    "\n",
    "$$d=1, \\quad \\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "to\n",
    "\n",
    "$$d=10, \\quad \\hat{y} = w_0 + w_1 x + w_2 x^2 + \\ldots + w_{10} x^{10}$$\n",
    "\n",
    "We could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "# loop over the folds\n",
    "# the first loop variable tells us how many out of nfold folds we have gone through\n",
    "# the second loop variable tells us how to split the data\n",
    "for isplit, idx in enumerate(kf.split(x_train)):\n",
    "  # these are the indices for the training and validation indices\n",
    "  # for this iteration of the k folds\n",
    "  idx_tr, idx_val = idx\n",
    "  x_train_kfold = x_train_trans[idx_tr]\n",
    "  y_train_kfold = y_train[idx_tr]\n",
    "  x_val_kfold = x_train_trans[idx_val]\n",
    "  y_val_kfold = y_train[idx_val]\n",
    "  for didx, dtest in enumerate(dtest_list):\n",
    "    x_train_dtest =  x_train_kfold[:, :dtest]\n",
    "    x_val_dtest   =  x_val_kfold[:, :dtest]\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    y_hat = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "target = mse_val[idx_min,:].mean() + mse_val[idx_min,:].std()/np.sqrt(nfold-1)\n",
    "idx_one_se = np.where(mse_val.mean(axis=1) <= target)\n",
    "d_one_se = np.min(dtest_list[idx_one_se])\n",
    "d_one_se\n",
    "# r2\n",
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "target_r2 = r2_val[idx_max,:].mean() - r2_val[idx_max,:].std()/np.sqrt(nfold-1)\n",
    "idx_one_se_r2 = np.where(r2_val.mean(axis=1) >= target_r2)\n",
    "d_one_se_r2 = np.min(dtest_list[idx_one_se_r2])\n",
    "d_one_se_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_poly = df_tr.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)\n",
    "df_ts_poly = df_ts.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)\n",
    "reg_covid = LinearRegression().fit(df_tr_poly, df_tr.deathIncrease)\n",
    "deathIncrease_fitted = reg_covid.predict(df_tr_poly)\n",
    "metrics.r2_score(df_tr.deathIncrease, deathIncrease_fitted)\n",
    "dmax = 4\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "idxval = [10, 20, 30, 40, 50]\n",
    "nfold = len(idxval)\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "mse_tr  = np.zeros((nd,nfold))\n",
    "r2_tr   = np.zeros((nd,nfold))\n",
    "for isplit, idx in enumerate(idxval):\n",
    "    x_train_kfold = df_tr_poly[:idx]\n",
    "    y_train_kfold = df_tr.deathIncrease.values[:idx]\n",
    "    x_val_kfold = df_tr_poly[idx:idx+10]\n",
    "    y_val_kfold = df_tr.deathIncrease.values[idx:idx+10]\n",
    "    for didx, dtest in enumerate(dtest_list):\n",
    "      x_train_dtest =  x_train_kfold[:, :dtest]\n",
    "      x_val_dtest   =  x_val_kfold[:, :dtest]\n",
    "      reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "      y_hat = reg_dtest.predict(x_val_dtest)\n",
    "      mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "      r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)\n",
    "      y_hat_tr = reg_dtest.predict(x_train_dtest)\n",
    "      mse_tr[didx, isplit] = metrics.mean_squared_error(y_train_kfold, y_hat_tr)\n",
    "      r2_tr[didx, isplit] = metrics.r2_score(y_train_kfold, y_hat_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\n",
    "y_names = ['price']\n",
    "random_state = 13\n",
    "Xtr_df, Xts_df = train_test_split(df[x_names], test_size=0.3, random_state=random_state, shuffle=True)\n",
    "ytr_df, yts_df = train_test_split(df[y_names], test_size=0.3, random_state=random_state, shuffle=True)\n",
    "Xtr, Xts, ytr, yts = np.array(Xtr_df), np.array(Xts_df), np.array(ytr_df), np.array(yts_df)\n",
    "alpha_list = np.array([0, 10, 20, 50, 100, 200, 500])\n",
    "nfold = 5\n",
    "mse_val = np.zeros((len(alpha_list), nfold))\n",
    "# k-ford\n",
    "kf = KFold(n_splits=nfold, shuffle=False)\n",
    "# For each fold, standardize the data\n",
    "for ifold, (idx_tr, idx_val) in enumerate(kf.split(Xtr)):\n",
    "    X_train_fold, X_val_fold = Xtr[idx_tr], Xtr[idx_val]\n",
    "    y_train_fold, y_val_fold = ytr[idx_tr], ytr[idx_val]\n",
    "    scaler = StandardScaler().fit(X_train_fold)\n",
    "    X_train_fold_std = scaler.transform(X_train_fold)\n",
    "    X_val_fold_std = scaler.transform(X_val_fold)\n",
    "    # For each alpha in the list, fit a Ridge regression model on the standardized data\n",
    "    for i, alpha in enumerate(alpha_list):\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X_train_fold_std, y_train_fold)\n",
    "        y_pred = model.predict(X_val_fold_std)\n",
    "        # update the appropriate entry in mse_val\n",
    "        mse_val[i, ifold] = mean_squared_error(y_val_fold, y_pred)\n",
    "mse_mean = np.mean(mse_val, axis=1)\n",
    "alpha_min_mse = alpha_list[np.argmin(mse_mean)]\n",
    "# entire training set\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_std = scaler.transform(Xtr)\n",
    "Xts_std = scaler.transform(Xts)\n",
    "model = Ridge(alpha=alpha_min_mse)\n",
    "model.fit(Xtr_std, ytr)\n",
    "y_pred = model.predict(Xts_std)\n",
    "mse_ridge = mean_squared_error(yts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize data in each fold.\n",
    "random_state = 14\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "med_values = np.nanmedian(Xtr, axis=0)\n",
    "Xtr_filled = np.nan_to_num(Xtr, nan=med_values)\n",
    "Xts_filled = np.nan_to_num(Xts, nan=med_values)\n",
    "C_test = np.logspace(-1,3,10)\n",
    "nfold = 3\n",
    "acc_val = np.zeros((len(C_test), nfold))\n",
    "for iC, C in enumerate(C_test): \n",
    "    kf = KFold(n_splits=nfold, shuffle=False)\n",
    "    for ifold, (Itr, Ival) in enumerate(kf.split(Xtr_filled)):\n",
    "        Xtr_fold, Xval_fold = Xtr_filled[Itr], Xtr_filled[Ival]\n",
    "        ytr_fold, yval_fold = ytr[Itr], ytr[Ival]     \n",
    "        scaler = StandardScaler().fit(Xtr_fold)\n",
    "        Xtr_std = scaler.transform(Xtr_fold)\n",
    "        Xvl_std = scaler.transform(Xval_fold)\n",
    "        clf = LogisticRegression(random_state = random_state, solver = 'liblinear', penalty='l1', C = C)\n",
    "        clf.fit(Xtr_std, ytr_fold)\n",
    "        yhat = clf.predict(Xvl_std)\n",
    "        acc_val[iC, ifold] = accuracy_score(yval_fold, yhat)\n",
    "acc_mean = np.mean(acc_val, axis=1)\n",
    "C_best = C_test[np.argmax(acc_mean)]\n",
    "# entire training set\n",
    "scaler = StandardScaler().fit(Xtr_filled)\n",
    "Xtr_std = scaler.transform(Xtr_filled)\n",
    "Xts_std = scaler.transform(Xts_filled)\n",
    "clf_best = LogisticRegression(random_state = random_state, solver = 'liblinear', penalty='l1', C = C_best)\n",
    "clf_best.fit(Xtr_std, ytr)\n",
    "y_hat = clf_best.predict(Xts_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_str, Xts_str, ytr, yts = train_test_split(df['statement'].values, df['label_binary'].values, shuffle=True, random_state=0, test_size=0.25)\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "Xtr_vec = vec.fit_transform(Xtr_str)\n",
    "Xts_vec = vec.transform(Xts_str)\n",
    "C_test = np.logspace(-3, 3, num=20)\n",
    "nfold = 5\n",
    "acc_val = np.zeros((len(C_test), nfold))\n",
    "kf = KFold(n_splits=nfold)\n",
    "for ifold, (Itr, Ival) in enumerate(kf.split(Xtr_vec)):\n",
    "    for iC, C in enumerate(C_test):\n",
    "        clf = LogisticRegression(random_state = 0, penalty='l1', solver='liblinear', C = C)\n",
    "        clf.fit(Xtr_vec[Itr], ytr[Itr])\n",
    "        yhat = clf.predict(Xtr_vec[Ival])\n",
    "        acc_val[iC, ifold] = accuracy_score(ytr[Ival], yhat)\n",
    "acc_mean = np.mean(acc_val, axis=1)\n",
    "C_best = C_test[np.argmax(acc_mean)]\n",
    "model_best = LogisticRegression(penalty='l1', C=C_best, random_state=0, solver='liblinear')\n",
    "model_best.fit(Xtr_vec, ytr)\n",
    "y_pred_best = model_best.predict(Xts_vec)\n",
    "acc_best = accuracy_score(yts, y_pred_best)\n",
    "count_best = np.count_nonzero(model_best.coef_)\n",
    "acc_std = acc_val.std(axis=1)\n",
    "acc_one_se = acc_mean - acc_std\n",
    "C_one_se = C_test[np.argmax(acc_mean >= np.max(acc_one_se))]\n",
    "model_one_se = LogisticRegression(penalty='l1', C=C_one_se, random_state=0, solver='liblinear')\n",
    "model_one_se.fit(Xtr_vec, ytr)\n",
    "y_pred_one_se = model_one_se.predict(Xts_vec)\n",
    "acc_one_se = accuracy_score(yts, y_pred_one_se)\n",
    "count_one_se = np.count_nonzero(model_one_se.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold CV with Fourier basis expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You decide to use a linear regression model with Fourier basis transformation of the `hourofweek` feature:\n",
    "\n",
    "$$\\hat{y}=w_0 + w_1 x +  \\sum_{t \\in \\text{tlist}} w_{t,c} \\cos(2\\pi x/t)+w_{t,s} \\sin(2\\pi x/t)$$\n",
    "\n",
    "where each sine and cosine pair represents the periodic behavior over a particular time interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if `tlist = [0.5, 1]`, then your model would be:\n",
    "\n",
    "$$\\hat{y}=w_0 + w_1 x + w_{0.5,c} \\cos(2\\pi x/0.5)+w_{0.5,s} \\sin(2\\pi x/0.5) + w_{1,c} \\cos(2\\pi x/1)+w_{1,s} \\sin(2\\pi x/1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to fit a K-fold CV! In your CV, you will fit and evaluate a `LinearRegression` model (using the `sklearn` implementation) on an increasing number of columns of the data, as described above - \n",
    "\n",
    "* in the first iteration of your K-fold CV, you will evaluate the regression for `tlist_eval = []`. \n",
    "* In the second iteration of your K-fold CV, you will evaluate the regression for `tlist_eval = [1]`.\n",
    "* In the third iteration, you will evaluate the regression for `tlist_eval = [1, 2]`\n",
    "\n",
    "and so on, until, in the final iteration, you will evaluate the regression for *all* the values in `tlist`.\n",
    "\n",
    "(Of course, you won't re-compute the Fourier basis transformation inside the loop - you'll just select the appropriate rows and columns from `Xtr_trans` in each iteration.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have prepared a \"ones column\" in the data, you will pass `fit_intercept=False` as an argument to the `LinearRegression`, so that it won't also fit another \"intercept\" term (in addition to the coefficient for the \"ones column\".)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "df_tr, df_ts = train_test_split(df, train_size=10000, shuffle=False, random_state=42)\n",
    "features = ['hour', 'month', 'dayofweek', 'hourofweek']\n",
    "target = 'ridership'\n",
    "model = LinearRegression()\n",
    "model.fit(df_tr[features], df_tr[target])\n",
    "y_pred = model.predict(df_ts[features])\n",
    "r2_lr = r2_score(df_ts[target], y_pred)\n",
    "# K-fold CV with fourier basis expansion\n",
    "tlist = np.arange(1, 51)\n",
    "Xtr = df_tr['hourofweek'].values\n",
    "Xts = df_ts['hourofweek'].values\n",
    "ytr = df_tr['ridership'].values\n",
    "yts = df_ts['ridership'].values\n",
    "Xtr_trans = np.column_stack((np.ones_like(Xtr), Xtr))\n",
    "Xts_trans = np.column_stack((np.ones_like(Xts), Xts))\n",
    "for t in tlist:\n",
    "    cos_tr = np.cos(2 * np.pi * Xtr / t)\n",
    "    sin_tr = np.sin(2 * np.pi * Xtr / t)\n",
    "    cos_ts = np.cos(2 * np.pi * Xts / t)\n",
    "    sin_ts = np.sin(2 * np.pi * Xts / t)\n",
    "    Xtr_trans = np.column_stack((Xtr_trans, cos_tr, sin_tr))\n",
    "    Xts_trans = np.column_stack((Xts_trans, cos_ts, sin_ts))\n",
    "Xtr_trans = np.round(Xtr_trans, 10)\n",
    "Xts_trans = np.round(Xts_trans, 10)\n",
    "\n",
    "nfold = 5\n",
    "r2_val = np.zeros((len(tlist) + 1, nfold))\n",
    "kf = KFold(n_splits=nfold, shuffle=False)\n",
    "for i, t in enumerate(range(len(tlist) + 1)):\n",
    "    # The first model uses only the intercept and original X (2 columns)\n",
    "    # Subsequent models add Fourier features in pairs (cos and sin)\n",
    "    num_columns = 2 + 2 * i \n",
    "    X_subset = Xtr_trans[:, :num_columns]\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_subset)):\n",
    "            X_train, X_val = X_subset[train_idx], X_subset[val_idx]\n",
    "            y_train, y_val = ytr[train_idx], ytr[val_idx]\n",
    "            model = LinearRegression(fit_intercept=False)\n",
    "            model.fit(X_train, y_train)           \n",
    "            y_pred = model.predict(X_val)\n",
    "            r2_val[i, fold] = r2_score(y_val, y_pred)\n",
    "\n",
    "r2_mean = r2_val.mean(axis=1)\n",
    "r2_se = np.std(r2_val, axis=1, ddof=1) / np.sqrt(nfold)\n",
    "\n",
    "idx_max = np.argmax(r2_mean)\n",
    "# Compute the threshold for the one-SE rule\n",
    "one_se = r2_mean[idx_max] - r2_se[idx_max]\n",
    "# Find the simplest model (smallest number of Fourier features) whose RÂ² is within one SE of the best model\n",
    "tlist_opt = []\n",
    "for i in range(len(r2_mean)):\n",
    "    if r2_mean[i] >= one_se:\n",
    "        tlist_opt = tlist[:i]  # Select the first i values of tlist\n",
    "        break\n",
    "# Train a model on the entire training set for this `tlist_opt`, then evaluate its performance on the test set. \n",
    "# Save the test R2 score in r2_one_se.\n",
    "num_columns = 2 + 2 * len(tlist_opt)\n",
    "Xtr_opt = Xtr_trans[:, :num_columns]\n",
    "Xts_opt = Xts_trans[:, :num_columns]\n",
    "model_opt = LinearRegression(fit_intercept=False)\n",
    "model_opt.fit(Xtr_opt, ytr)\n",
    "y_pred_opt = model_opt.predict(Xts_opt)\n",
    "r2_one_se = r2_score(yts, y_pred_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting and Preventing Deaths in the ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "X = df[col_names[:-1]]\n",
    "y = df['In-hospital_death'].values \n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=800, shuffle=True, random_state=0)\n",
    "Xtr, Xvl, ytr, yvl = train_test_split(Xtr, ytr, test_size=800, shuffle=True, random_state=0)\n",
    "col_meds = Xtr.median()\n",
    "Xtr = Xtr.fillna(col_meds)\n",
    "Xvl = Xvl.fillna(col_meds)\n",
    "Xts = Xts.fillna(col_meds)\n",
    "# Logistic Regression\n",
    "clf = LogisticRegression(random_state = 0, penalty=None)\n",
    "clf.fit(Xtr, ytr)\n",
    "y_pred_lr = clf.predict(Xvl)\n",
    "acc_lr = accuracy_score(yvl, y_pred_lr)\n",
    "most_frequent_label = np.bincount(ytr).argmax()\n",
    "y_pred_base = np.full_like(yvl, most_frequent_label)\n",
    "acc_base = accuracy_score(yvl, y_pred_base)\n",
    "# ROC curve\n",
    "y_proba = clf.predict_proba(Xvl)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(yvl, y_proba)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the doctor spends $t$ minutes reviewing a file, the probability of *missing* the correct treatment is\n",
    "\n",
    "$$P(\\textrm{fail}) = \\exp(-t^2/100).$$\n",
    "\n",
    "## Policy 0: No extra review\n",
    "## Policy 1: Review all files\n",
    "Now suppose the specialist team will spend\n",
    "$$\\frac{N}{25} \\times 60$$\n",
    "minutes total on extra review, where $N$ is the number of files in the validation set, and they will spend an equal amount of time on each of the $N$ files in the validation set.\n",
    "## Policy 2: Review high-risk files with 0.5 threshold\n",
    "Now, suppose that the doctor only reviews the files of patients for whom the model outputs a probability of mortality greater than 50%. They will spend the same total amount of time as before, \n",
    "$$\\frac{N}{25} \\times 60$$\n",
    "Compute:\n",
    "* `t_2` - the time (in minutes) spent on each file\n",
    "* `p_2` - the probability of failing to identify the correct intervention, given this `t`\n",
    "and then use them to find the expected number of deaths in the validation set, with this policy. Save this value in `mort_2`.\n",
    "## Policy 3: Review high-risk files with optimal threshold\n",
    "For all thresholds in `np.arange(0, 1, 0.01)`, compute the expected mortality, given the doctor only reviews the files of patients for whom the model outputs a probability of mortality greater than that threshold. They will spend the same total amount of time as before, \n",
    "$$\\frac{N}{25} \\times 60$$\n",
    "minutes total on extra review, where $N$ is the number of files in the validation set. \n",
    "Save the results in `mort_thr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy 0: No extra review\n",
    "mort_0 = np.sum(yvl)\n",
    "\n",
    "# Policy 1: Review all files\n",
    "N = len(yvl)\n",
    "# Step 1: Compute time spent per file\n",
    "t_1 = (N / 25) * 60  / N  # Minutes per file\n",
    "# Step 2: Compute probability of missing correct intervention\n",
    "p_1 = np.exp(-t_1**2 / 100)\n",
    "# Step 3: Compute expected mortality under Policy 1\n",
    "mort_1 = np.sum(yvl * p_1)\n",
    "# Step 1: Predict probabilities of mortality for the validation set\n",
    "\n",
    "# Policy 2: Review high-risk files with 0.5 threshold\n",
    "y_proba_vl = clf.predict_proba(Xvl)[:, 1]\n",
    "# Identify high-risk and low-risk patients\n",
    "high_risk_indices = [i for i in range(len(yvl)) if y_proba_vl[i] > 0.5]\n",
    "low_risk_indices = [i for i in range(len(yvl)) if y_proba_vl[i] <= 0.5]\n",
    "# Number of high-risk files\n",
    "N_high_risk = len(high_risk_indices)\n",
    "# Step 2: Calculate t_2\n",
    "N = len(yvl)\n",
    "T = (N / 25) * 60\n",
    "t_2 = T / N_high_risk\n",
    "# Step 3: Calculate p_2\n",
    "p_2 = np.exp(-t_2**2 / 100)\n",
    "# Step 4: Estimate expected deaths\n",
    "mort_2 = 0\n",
    "for i in high_risk_indices:\n",
    "    if yvl[i] == 1:  # Patient would have died without review\n",
    "        mort_2 += p_2  # Adjusted probability of death\n",
    "    else:\n",
    "        mort_2 += 0  # No death for survivors\n",
    "for i in low_risk_indices:\n",
    "    mort_2 += yvl[i]  # No review, so deaths remain unchanged\n",
    "    \n",
    "# Policy 3: Review high-risk files with optimal threshold\n",
    "# Step 1: Predict probabilities of mortality for the validation set\n",
    "y_proba_vl = clf.predict_proba(Xvl)[:, 1]  # Predicted probabilities for validation set\n",
    "# Total number of files in the validation set\n",
    "N = len(yvl)\n",
    "# Total time spent on extra review (same as in Policy 1 and Policy 2)\n",
    "T = (N / 25) * 60\n",
    "# Step 2: Initialize arrays to store results\n",
    "thr_list = np.arange(0, 1, 0.01)  # List of thresholds\n",
    "mort_thr = np.zeros(len(thr_list))  # Array to store expected mortality for each threshold\n",
    "# Step 3: Iterate over thresholds\n",
    "for i, t in enumerate(thr_list):\n",
    "    # Identify high-risk patients (probability > threshold)\n",
    "    high_risk_indices = np.where(y_proba_vl > t)[0]\n",
    "    low_risk_indices = np.where(y_proba_vl <= t)[0]\n",
    "    # Number of high-risk files\n",
    "    N_high_risk = len(high_risk_indices)\n",
    "    # Skip if there are no high-risk patients (to avoid division by zero)\n",
    "    if N_high_risk == 0:\n",
    "        mort_thr[i] = np.sum(yvl[low_risk_indices])  # No review, so deaths remain unchanged\n",
    "        continue\n",
    "    # Calculate time spent on each high-risk file\n",
    "    t_2_3 = T / N_high_risk\n",
    "    # Calculate probability of failing to identify the correct intervention\n",
    "    p_2 = np.exp(-t_2_3**2 / 100) \n",
    "    # Estimate expected deaths\n",
    "    mort_high_risk = np.sum(yvl[high_risk_indices] * p_2)  # Adjusted probability for high-risk patients\n",
    "    mort_low_risk = np.sum(yvl[low_risk_indices])  # No review for low-risk patients\n",
    "    # Total expected deaths\n",
    "    mort_thr[i] = mort_high_risk + mort_low_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find \n",
    "\n",
    "* `thr_opt`, the threshold that minimizes mortality on the validation set, and \n",
    "* `mort_3`, the expected mortality on the validation set with this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_opt = thr_list[np.argmin(mort_thr)]\n",
    "# mort_3 = np.min(mort_thr)\n",
    "mort_3 = np.min(mort_thr[mort_thr > 0]) if np.any(mort_thr > 0) else np.min(mort_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_ts = np.sum(yts)\n",
    "# Policy 0: no review, so no lives are saved\n",
    "saved_0 = 0\n",
    "# Policy 1: review all\n",
    "N = len(yts)\n",
    "total_time = (N / 25) * 60\n",
    "t_ts_1 = total_time / N\n",
    "p_ts_1 = np.exp(-(t_ts_1 ** 2) / 100)\n",
    "at_risk_patients = np.sum(yts)\n",
    "mort_ts_1 = at_risk_patients * p_ts_1\n",
    "saved_1 = mort_ts - mort_ts_1\n",
    "# Policy 2: review using 50% threshold\n",
    "y_proba_ts = clf.predict_proba(Xts)[:, 1]\n",
    "high_risk_patients = y_proba_ts > 0.5\n",
    "num_high_risk = np.sum(high_risk_patients)\n",
    "t_ts_2 = total_time / num_high_risk if num_high_risk > 0 else 0\n",
    "p_ts_2 = np.exp(-(t_ts_2 ** 2) / 100)\n",
    "at_risk_high_risk = np.sum(yts[high_risk_patients])\n",
    "at_risk_low_risk = np.sum(yts[~high_risk_patients])\n",
    "mort_ts_2 = at_risk_high_risk * p_ts_2 + at_risk_low_risk\n",
    "saved_2 = mort_ts - mort_ts_2\n",
    "# Policy 3: review using thr_opt\n",
    "high_risk_patients = y_proba_ts > thr_opt\n",
    "num_high_risk = np.sum(high_risk_patients)\n",
    "t_ts_3 = total_time / num_high_risk if num_high_risk > 0 else 0\n",
    "p_ts_3 = np.exp(-(t_ts_3 ** 2) / 100)\n",
    "at_risk_high_risk = np.sum(yts[high_risk_patients])\n",
    "at_risk_low_risk = np.sum(yts[~high_risk_patients])\n",
    "mort_ts_3 = at_risk_high_risk * p_ts_3 + at_risk_low_risk\n",
    "saved_3 = mort_ts - mort_ts_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
