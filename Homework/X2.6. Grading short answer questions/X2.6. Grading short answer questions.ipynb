{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short answer grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Professor Fund likes teaching, but she does not like grading student's work - she would much rather spend that time interacting directly with students.\n",
    "\n",
    "You are going to help her out by developing a machine learning model to make this arduous task easier, at least for short answer questions.\n",
    "\n",
    "In the attached workspace, you will load in a dataset related to short answer grading, then explore some models that you think could streamline this arduous task.\n",
    "\n",
    "| Name | Type | Description |\n",
    "| ---- | ---- | ---- |\n",
    "|`acc_rf`\t|1d numpy array\t|1. Accuracy of random forest for each question ID.|\n",
    "|`acc_rf_kf`\t|2d numpy array\t|2. Accuracy of random forest for each question ID in each fold.|\n",
    "|`acc_rf_mean`\t|1d numpy array\t|2. Mean accuracy of random forest for each question ID across folds.|\n",
    "|`cluster_ids`\t|1d numpy array\t|3. Cluster ID for each sample.|\n",
    "|`cluster_correctness`\t|2d numpy array\t|3. Percent of samples in each cluster with correct answer.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This question includes bonus points - you only need to earn 20 points of credit to get full credit, but you _can_ earn up to 30 points (and therefore, up to 110% on the overall exam). You may work on whichever sections you like to earn the first 20 points, but don't spend time trying to get to 30 points until you have first answered the rest of the exam.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this `random_state` throughout your work: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are working with a data set that has four different short answer questions, each with a reference answer and approximately 30 student answers.\n",
    "\n",
    "First, read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"id\"].unique() # four question IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Carrie wanted to find out which was harder, a penny or a nickel, so she did a scratch test. How would this tell her which is harder?',\n",
       "       'Look at the picture on the right. Label the poles on each magnet. (The bottom 2 magnets are stuck together, the others are not.) What is the rule that explains why you labeled the poles the way you did?',\n",
       "       'A solution is a type of mixture. What makes it different from other mixtures?',\n",
       "       'Katie got a guitar for her birthday. She experimented with the strings and found she could change their sounds. One way Katie could change the sound of a string was to tighten it. Describe how the sound was different when the string was tightened.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"question\"].unique() # four questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The harder coin will scratch the other.',\n",
       "       'Like poles repel and opposite poles attract.',\n",
       "       'A solution is a mixture formed when a solid dissolves in a liquid.',\n",
       "       'When the string was tighter, the pitch was higher.'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"reference_answer\"].unique() # four reference answers (the \"official\" correct answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>correct</th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You could tell if it has the hardest if most o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Carrie wanted to find out which was harder, a ...</td>\n",
       "      <td>The harder coin will scratch the other.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If just the penny could scratch and the nickel...</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie wanted to find out which was harder, a ...</td>\n",
       "      <td>The harder coin will scratch the other.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whichever one was damaged most was less hard.</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie wanted to find out which was harder, a ...</td>\n",
       "      <td>The harder coin will scratch the other.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rub them against a crystal.</td>\n",
       "      <td>0</td>\n",
       "      <td>Carrie wanted to find out which was harder, a ...</td>\n",
       "      <td>The harder coin will scratch the other.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which had less scratches.</td>\n",
       "      <td>1</td>\n",
       "      <td>Carrie wanted to find out which was harder, a ...</td>\n",
       "      <td>The harder coin will scratch the other.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  correct  \\\n",
       "0  You could tell if it has the hardest if most o...        0   \n",
       "1  If just the penny could scratch and the nickel...        1   \n",
       "2      Whichever one was damaged most was less hard.        1   \n",
       "3                        Rub them against a crystal.        0   \n",
       "4                          Which had less scratches.        1   \n",
       "\n",
       "                                            question  \\\n",
       "0  Carrie wanted to find out which was harder, a ...   \n",
       "1  Carrie wanted to find out which was harder, a ...   \n",
       "2  Carrie wanted to find out which was harder, a ...   \n",
       "3  Carrie wanted to find out which was harder, a ...   \n",
       "4  Carrie wanted to find out which was harder, a ...   \n",
       "\n",
       "                          reference_answer  id  \n",
       "0  The harder coin will scratch the other.   0  \n",
       "1  The harder coin will scratch the other.   0  \n",
       "2  The harder coin will scratch the other.   0  \n",
       "3  The harder coin will scratch the other.   0  \n",
       "4  The harder coin will scratch the other.   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # note many answers per question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a RandomForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train and evaluate a separate model for each question, so in everything you do, you will start by iterating over the question IDs.\n",
    "\n",
    "Then, for each question ID:\n",
    "\n",
    "* get the data for that question ID\n",
    "* divide the data into a training and test set, using the random state specified above, and leaving 10 random samples per question in the test set with the rest in the training set.\n",
    "* Use a `CountVectorizer` with `stop_words = \"english\"` to create a numeric representation of the answers to that question, using the answer data in the training set to fit the `CountVectorizer`.\n",
    "* train a `RandomForestClassifier` to predict whether or not the answer is correct. Use 20 trees in your forest, and use the random state specified above.\n",
    "* save the result on the validation set (per question ID) in `acc_rf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.7 0.8 0.8]\n"
     ]
    }
   ],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "question_ids = df[\"id\"].unique()\n",
    "acc_rf = np.zeros(len(question_ids))\n",
    "# implement here\n",
    "for i, question_id in enumerate(question_ids):\n",
    "    question_data = df[df[\"id\"] == question_id]\n",
    "    X = question_data[\"answer\"]\n",
    "    y = question_data[\"correct\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10, random_state=random_state)\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    model = RandomForestClassifier(n_estimators=20, random_state=random_state)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    acc_rf[i] = accuracy_score(y_test, y_pred)\n",
    "print(acc_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate a RandomForest with KFold CV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you are working with a very small data set, you are concerned that perhaps the results are highly dependent on the random draw of training vs test samples. So, you repeat the analysis above, but with a KFold CV for evaluation:\n",
    "\n",
    "* first, you iterate over question IDs\n",
    "* then, you set up a KFold CV with 5 folds. Shuffle the data and use the random state specified above.\n",
    "* inside each fold, \n",
    "  * Use a `CountVectorizer` with `stop_words = \"english\"` to create a numeric representation of the answers to that question, using the training set to fit the `CountVectorizer`.\n",
    "  * train a `RandomForestClassifier` to predict whether or not the answer is correct. Use 20 trees in your forest, and use the random state specified above.\n",
    "  * save the result on the validation set (per question ID) in `acc_rf_kf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "question_ids = df[\"id\"].unique()\n",
    "n_folds = 5\n",
    "acc_rf_kf = np.zeros((len(question_ids), n_folds))\n",
    "# implement here\n",
    "for i, question_id in enumerate(question_ids):\n",
    "    question_data = df[df[\"id\"] == question_id]\n",
    "    X = question_data[\"answer\"]\n",
    "    y = question_data[\"correct\"]\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    for j, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "        model = RandomForestClassifier(n_estimators=20, random_state=random_state)\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        acc_rf_kf[i, j] = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, get the average accuracy per question ID (across folds) and save it in `acc_rf_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61071429 0.81428571 0.77857143 0.74642857]\n"
     ]
    }
   ],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "acc_rf_mean = np.mean(acc_rf_kf, axis=1)\n",
    "print(acc_rf_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering similar answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised learning model above is interesting, but is only useful for questions on which Professor Fund already has labeled data from previous semesters. It won't help her grade new questions that have not been used in previous semesters.\n",
    "\n",
    "To help with brand-new questions, you will also create a supervised learning model that will group together similar answers, making them easier to grade."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, \n",
    "\n",
    "* first, you iterate over question IDs. For each question, \n",
    "  * Use a `CountVectorizer` with `stop_words = \"english\"` to create a numeric representation of the answers to that question, using the answer data to fit the `CountVectorizer`.\n",
    "  * then use a `KMeans` model with 3 clusters to group similar answers. Use the random state specified above, and leave all other settings at their default values.\n",
    "  * save the cluster ID of each sample in `cluster_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "cluster_ids = np.zeros(len(df), dtype=int)\n",
    "# implement here\n",
    "for i, question_id in enumerate(question_ids):\n",
    "    question_data = df[df[\"id\"] == question_id]\n",
    "    X = question_data[\"answer\"]\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X_vec = vectorizer.fit_transform(X)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=random_state)\n",
    "    cluster_ids[df[\"id\"] == question_id] = kmeans.fit_predict(X_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your clustering model is doing a good job of grouping together answers that will be graded similarly, we would expect that within a cluster, most of the answers should be either correct or incorrect (i.e. most answers within a cluster should have similar \"correctness\".)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question and each cluster, compute the average \"correctness\" within the cluster (i.e. the average value of \"correct\" among samples within that cluster). Save the result in `cluster_correctness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.57142857 0.4       ]\n",
      " [0.05882353 0.75       0.14285714]\n",
      " [0.23333333 0.         0.5       ]\n",
      " [0.38095238 0.57142857 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "question_ids = df[\"id\"].unique()\n",
    "n_clusters = 3  \n",
    "cluster_correctness = np.zeros((len(question_ids), n_clusters))\n",
    "# implement here\n",
    "for i, question_id in enumerate(question_ids):\n",
    "    question_data = df[df[\"id\"] == question_id]\n",
    "    for j in range(n_clusters):\n",
    "        cluster_data = question_data[cluster_ids[df[\"id\"] == question_id] == j]\n",
    "        if len(cluster_data) > 0:\n",
    "            cluster_correctness[i, j] = cluster_data[\"correct\"].mean()\n",
    "print(cluster_correctness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
