{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXxolfTMTVbe"
      },
      "source": [
        "# Logistic regression in depth\n",
        "\n",
        "*Fraida Fund*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO67fFXeTVbk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# for 3d plots\n",
        "from ipywidgets import interact, fixed\n",
        "from mpl_toolkits import mplot3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHwfDOkmTVbl"
      },
      "source": [
        "## Basic logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G03gj3exTVbn"
      },
      "source": [
        "### Data for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSA2cQaGTVbn"
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\n",
        "w_true = [-1, 1.2, 1]\n",
        "sigma = 0.1\n",
        "plot_colors = np.array(sns.color_palette().as_hex())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfJOwrPaTVbo"
      },
      "outputs": [],
      "source": [
        "# Generate training data\n",
        "X = np.random.uniform(0, 1, size=(n_samples,2))\n",
        "y = np.array(w_true[0]+w_true[1]*X[:,0]+w_true[2]*X[:,1] >= 0).astype(int)\n",
        "\n",
        "# Add some noise?\n",
        "X = X + sigma*np.random.randn(X.shape[0]*X.shape[1]).reshape(X.shape[0], X.shape[1])\n",
        "\n",
        "# Figure formatting stuff\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');\n",
        "x_true = np.linspace(0, 1)\n",
        "y_true =  -(x_true * w_true[1] + w_true[0])/w_true[2]\n",
        "sns.lineplot(x=x_true, y=y_true, color='black', label='True separating hyperplane');\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJsN9VKMTVbo"
      },
      "source": [
        "### Learning weights using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3I3N6bHTVbq"
      },
      "source": [
        "The logistic regression learns the coefficient vector $w$ to minimize the loss function\n",
        "\n",
        "$$L(w) = \\sum_{i=1}^n - \\left( y_i \\log \\frac{1}{1 + e^{- \\langle w, x_i \\rangle}} + (1 - y_i) \\log \\frac{e^{- \\langle w, x_i \\rangle}}{1 + e^{- \\langle w, x_i \\rangle}} \\right) $$\n",
        "\n",
        "(Assume that the data matrix has a column of 1s at the beginning, so that the intercept can be learned the same way as the other coefficients)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAxhSSoQTVbq"
      },
      "source": [
        "We can use gradient descent to learn the intercept and coefficient vector, using the gradient update rule\n",
        "\n",
        "$$w_{k+1} = w_k + \\alpha \\sum_{i=1}^n (y_i - \\frac{1}{1 + e^{-\\langle w,x_i \\rangle}}) x_i ,$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETbUH_2YTVbq"
      },
      "outputs": [],
      "source": [
        "X_aug = np.hstack((np.ones((n_samples, 1)), X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ8LGr8VTVbr"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fisi4tt7TVbs"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(w, X, y):\n",
        "  p_pred = sigmoid(np.dot(X,w))\n",
        "  loss_pos = y*np.log(p_pred)\n",
        "  loss_neg = (1-y)*np.log(1-p_pred)\n",
        "  return np.sum(-1*(loss_pos + loss_neg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_TaRfe7TVbs"
      },
      "outputs": [],
      "source": [
        "def gd_step(w, X, y, lr):\n",
        "  p_pred = sigmoid(np.dot(X,w))\n",
        "  gradient = np.dot(X.T, y - p_pred)\n",
        "  w = w + lr * gradient\n",
        "  l = cross_entropy_loss(w,X,y)\n",
        "  return w, l, gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtyqGezPTVbs"
      },
      "outputs": [],
      "source": [
        "itr = 20000\n",
        "lr = 0.005\n",
        "tol = 0.01\n",
        "w_init = np.random.randn(3)\n",
        "print(w_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Oz6upMTVbt"
      },
      "outputs": [],
      "source": [
        "w_steps = np.zeros((itr, len(w_init)))\n",
        "l_steps = np.zeros(itr)\n",
        "grad_steps =  np.zeros((itr, len(w_init)))\n",
        "stop = 0\n",
        "\n",
        "w_star = w_init\n",
        "for i in range(itr):\n",
        "  w_star, loss, grad = gd_step(w_star, X_aug, y, lr)\n",
        "  w_steps[i] = w_star\n",
        "  l_steps[i] = loss\n",
        "  grad_steps[i] = grad\n",
        "  if np.linalg.norm(grad, ord=1) <= tol:\n",
        "    print(\"Stopping gradient descent at iteration %d\" % i)\n",
        "    stop = i\n",
        "    break\n",
        "  stop = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuAtDC1ITVbt"
      },
      "outputs": [],
      "source": [
        "print(w_star)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbnEZJfaTVbt"
      },
      "outputs": [],
      "source": [
        "# Figure formatting stuff\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');\n",
        "x_true = np.linspace(0, 1)\n",
        "y_true =  -(x_true * w_true[1] + w_true[0])/w_true[2]\n",
        "sns.lineplot(x=x_true, y=y_true, color='black', label='True separating hyperplane');\n",
        "y_fit =  -(x_true * w_star[1] + w_star[0])/w_star[2]\n",
        "sns.lineplot(x=x_true, y=y_fit, color='red', label='Fitted separating hyperplane');\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AzsCBcmTVbt"
      },
      "source": [
        "Why does the logistic regression choose *this* version of the separating hyperplane?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEhm9OFuTVbu"
      },
      "outputs": [],
      "source": [
        "colors = sns.color_palette(\"hls\", len(w_star))\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "\n",
        "plt.subplot(1,3,1);\n",
        "for n in range(len(w_star)):\n",
        "  sns.lineplot(x=np.arange(stop), y=w_steps[:stop,n], color=colors[n]);\n",
        "plt.xlabel(\"Iteration\");\n",
        "plt.ylabel(\"Coefficient Value\");\n",
        "\n",
        "plt.subplot(1,3, 2);\n",
        "sns.lineplot(x=np.arange(stop), y=l_steps[0:stop]);\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Iteration\");\n",
        "plt.ylabel(\"Cross Entropy Loss\");\n",
        "\n",
        "plt.subplot(1,3,3);\n",
        "for n in range(len(w_star)):\n",
        "  sns.lineplot(x=np.arange(stop), y=grad_steps[:stop,n], color=colors[n]);\n",
        "plt.xlabel(\"Iteration\");\n",
        "plt.ylabel(\"Gradient\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfc0kOSgTVbu"
      },
      "source": [
        "### Computing conditional probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceGGRqMGTVbu"
      },
      "source": [
        "The logistic regression learns weights, then for each point $x_{\\text{test}}$, it computes a conditional probability\n",
        "\n",
        "$$P(y_{\\text{test}} =1 | x = x_{\\text{test}}) =  \\frac{1}{1 + e^{-z}} $$\n",
        "\n",
        "where $z = w_0 + w_1 x_1 + w_2 x_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KHIcJ7ETVbu"
      },
      "outputs": [],
      "source": [
        "# Define the probability contours\n",
        "xx, yy = np.mgrid[0:1:.01, 0:1:.01]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "#z = clf.intercept_ + np.dot(grid, clf.coef_.T)\n",
        "z = w_star[0] + np.dot(grid, w_star[1:])\n",
        "probs = 1/(1+np.exp(-z)).reshape(xx.shape)\n",
        "\n",
        "# Figure formatting stuff\n",
        "fig = plt.figure()\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1);\n",
        "fig.colorbar(contours)\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ktx5P3dTVbu"
      },
      "source": [
        "Then, we can define a threshold. A common choice is 0.5, but we can choose any threshold we like, depending on the *cost* of different types of mistakes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sQEzTh5TVbu"
      },
      "outputs": [],
      "source": [
        "# Figure formatting stuff\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contour(xx, yy, probs, levels=[0.25, 0.5, 0.75], cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1);\n",
        "plt.clabel(contours, colors='black', inline=True, fontsize=10)\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='black', alpha=0.2);\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdtVW_I5TVbu"
      },
      "source": [
        "Which threshold would you choose:\n",
        "\n",
        "-   If you care most about overall accuracy (ratio of correct predictions to total predictions)?\n",
        "-   If you care most about avoiding false positives (labeling a point as positive, when it belongs to the negative class)?\n",
        "-   If you care most about avoiding false negatives (labeling a point as negative, when it belongs to the positive class)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2vq-BlaTVbv"
      },
      "source": [
        "Things to try:\n",
        "\n",
        "-   What happens as we increase $\\sigma$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8tg696JTVbv"
      },
      "source": [
        "This tradeoff is often visualized using the *receiver operating characteristic* and the overall performance of the classifier is evaluated using the *area under the \\[ROC\\] curve*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j0htmDATVbv"
      },
      "outputs": [],
      "source": [
        "y_hat = sigmoid(np.dot(X_aug,w_star))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-3QXmnTVbv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y, y_hat)\n",
        "roc_auc = auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jqjYVxqTVbv"
      },
      "outputs": [],
      "source": [
        "plt.figure();\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc);\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--');\n",
        "plt.xlim([0.0, 1.0]);\n",
        "plt.ylim([0.0, 1.05]);\n",
        "plt.xlabel('False Positive Rate');\n",
        "plt.ylabel('True Positive Rate');\n",
        "plt.title('ROC curve');\n",
        "plt.legend(loc=\"lower right\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwdzVMgVTVbv"
      },
      "source": [
        "### Using sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGIrrxQ3TVbv"
      },
      "source": [
        "Let us create a Logistic Regression classifier using `sklearn`, and see if it learns the same intercept and coefficient.\n",
        "\n",
        "Like other `sklearn` models, we train the Logistic Regression using its `fit()` method and then we can predict values using `predict()`. We can also get the conditional probabilities using `predict_proba()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y3iLXosTVbw"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(penalty=None,\n",
        "                         tol=0.01, solver='saga')\n",
        "clf.fit(X, y)\n",
        "print(clf.intercept_, clf.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13NZRK7zTVbw"
      },
      "outputs": [],
      "source": [
        "# Define the probability contours\n",
        "xx, yy = np.mgrid[0:1:.01, 0:1:.01]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Figure formatting stuff\n",
        "fig = plt.figure()\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1);\n",
        "fig.colorbar(contours)\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek181yMoTVbw"
      },
      "outputs": [],
      "source": [
        "# Figure formatting stuff\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "plt.xlim(0,1);\n",
        "plt.ylim(0,1);\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contour(xx, yy, probs, levels=3, cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1);\n",
        "plt.clabel(contours, colors='black', inline=True, fontsize=10)\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='black',  alpha=0.5);\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2XQacE-TVbw"
      },
      "source": [
        "## Data with non-linear decision boundary\n",
        "\n",
        "As with linear regression, we can use a logistic regression to classify data with a boundary that is linear, if we can find a transformed feature space with a linear boundary between classes.\n",
        "\n",
        "Consider the following data with a polynomial boundary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZFo8UdMTVbw"
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\n",
        "sigma = 0\n",
        "coefs=np.array([0.3, 1, -1.5, -2])\n",
        "xrange=[-1,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEN7CzdHTVbw"
      },
      "outputs": [],
      "source": [
        "def generate_polynomial_classifier_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
        "  x = np.random.uniform(xrange[0], xrange[1], size=(n, 2))\n",
        "  ysep = np.polynomial.polynomial.polyval(x[:,0],coefs)\n",
        "  y = (x[:,1]>ysep).astype(int)\n",
        "  x[:,0] = x[:,0] + sigma * np.random.randn(n)\n",
        "  x[:,1] = x[:,1] + sigma * np.random.randn(n)\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnz6SXywTVbx"
      },
      "outputs": [],
      "source": [
        "X, y = generate_polynomial_classifier_data(n=n_samples, xrange=xrange, coefs=coefs, sigma=sigma)\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y);\n",
        "\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2')\n",
        "\n",
        "\n",
        "# Plot true function\n",
        "xtrue = np.linspace(-1, 2)\n",
        "ytrue = np.polynomial.polynomial.polyval(xtrue,coefs)\n",
        "sns.lineplot(x=xtrue, y=ytrue, color='black')\n",
        "plt.xlim((xrange[0], xrange[1]));\n",
        "plt.ylim((xrange[0], xrange[1]));\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1WPVHATVb2"
      },
      "source": [
        "Our logistic regression can only learn linear boundaries, so it does not do very well on this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291ATDuQTVb2"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(penalty=None,\n",
        "                         tol=0.01, solver='saga')\n",
        "clf.fit(X, y)\n",
        "clf.score(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msTSuB0UTVb3"
      },
      "outputs": [],
      "source": [
        "# Define the probability contours\n",
        "xx, yy = np.mgrid[-1:1:.01, -1:1:.01]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Figure formatting stuff\n",
        "fig = plt.figure()\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1, alpha=0.5);\n",
        "fig.colorbar(contours)\n",
        "plt.contour(xx, yy, probs, levels=[0.5], colors='black', linestyles='dashed',\n",
        "                      vmin=0, vmax=1);\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');\n",
        "\n",
        "\n",
        "sns.lineplot(x=xtrue, y=ytrue, color='black')\n",
        "\n",
        "plt.xlim((xrange[0], xrange[1]));\n",
        "plt.ylim((xrange[0], xrange[1]));\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8d-YwKETVb4"
      },
      "source": [
        "(Here, the dashed black line shows the decision boundary, and the solid black line shows the true boundary.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpAB1jqXTVb4"
      },
      "source": [
        "But if we add $x_1^2$, $x_1^3$ to our model, then we can create a linear boundary between classes using a linear combination of features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ymp-SWPTVb4"
      },
      "outputs": [],
      "source": [
        "dmax = 3\n",
        "\n",
        "X_trans = np.hstack( [X[:,0].reshape(-1,1)**d for d in np.arange(0,dmax+1)] )\n",
        "X_trans = np.hstack((X_trans, X[:,1].reshape(-1,1)))\n",
        "X_trans.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUOaCHyvTVb4"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=np.dot(X_trans[:,0:dmax+1], coefs.reshape(-1,1)).squeeze(), y=X_trans[:,-1], hue=y)\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('y')\n",
        "plt.xlim((xrange[0], xrange[1]));\n",
        "plt.ylim((xrange[0], xrange[1]));\n",
        "\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIZeOh6TVb4"
      },
      "source": [
        "We can classify this data very well with a logistic regression on the transformed features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rczd2lJTVb4"
      },
      "outputs": [],
      "source": [
        "clf_trans = LogisticRegression(penalty=None,\n",
        "                         tol=0.01, solver='saga')\n",
        "clf_trans.fit(X_trans, y)\n",
        "clf_trans.score(X_trans, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VESuwPqJTVb4"
      },
      "outputs": [],
      "source": [
        "# Define the probability contours\n",
        "xx, yy = np.mgrid[-1:1:.01, -1:1:.01]\n",
        "grid = np.hstack([xx.reshape(-1,1)**d for d in np.arange(0, dmax+1)])\n",
        "grid = np.hstack([grid, yy.reshape(-1,1)])\n",
        "probs = clf_trans.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
        "\n",
        "# Figure formatting stuff\n",
        "fig = plt.figure()\n",
        "\n",
        "# Plot conditional probabilities\n",
        "contours = plt.contourf(xx, yy, probs, 25, cmap=\"RdBu_r\",\n",
        "                      vmin=0, vmax=1, alpha=0.5);\n",
        "fig.colorbar(contours)\n",
        "plt.contour(xx, yy, probs, levels=[0.5], colors='black', linestyles='dashed',\n",
        "                      vmin=0, vmax=1);\n",
        "\n",
        "# Plot training data\n",
        "sns.scatterplot(x=X[:,0], y=X[:,1], hue=y,\n",
        "                s=50, edgecolor='white');\n",
        "\n",
        "sns.lineplot(x=xtrue, y=ytrue, color='black', label='True polynomial separator')\n",
        "\n",
        "plt.xlim((xrange[0], xrange[1]));\n",
        "plt.ylim((xrange[0], xrange[1]));\n",
        "plt.xlabel('x1');\n",
        "plt.ylabel('x2');\n",
        "plt.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMR9Gi3HTVb5"
      },
      "source": [
        "As with linear regression:\n",
        "\n",
        "-   When there is under-modeling, adding transformed features can reduce bias.\n",
        "-   However, adding features also increases variance.\n",
        "-   We can use cross validation to choose a model that fits the data well but also generalizes.\n",
        "-   We can add a regularization penalty to our loss function to reduce variance."
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}