{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workspace, you will load in a dataset of gene expression data for classifying the leukemia type of samples. The data has 72 samples and every sample contains 7,129 gene expression values.\n",
    "\n",
    "You will train a linear SVC to classify these samples. However, since many of the features are not so relevant, you will first perform feature selection to identify the columns that are most closely correlated to the target variable. You'll use K-fold CV to select the number of samples to include in your SVC model.\n",
    "\n",
    "> Most of the points for this question are assigned automatically. However, a small number of points will additionally be manually assigned for code style: minimizing computation where possible, avoiding unnecessary for loops in favor of vectorized operations, etc.\n",
    "\n",
    "| Name | Type | Description |\n",
    "| ---- | ---- | ---- |\n",
    "|`Xtr`\t|2d numpy array\t|Training data (features).|\n",
    "|`Xts`\t|2d numpy array\t|Test data (features).|\n",
    "|`ytr`\t|1d numpy array\t|Training data (target).|\n",
    "|`yts`\t|1d numpy array\t|Test data (target).|\n",
    "|`score_ft`\t|2d numpy array\t|2d numpy array with score of each feature in each fold|\n",
    "|`score_val`\t|2d numpy array\t|2d numpy array with validation score (accuracy) of each model in each fold|\n",
    "|`best_d`\t|integer\t|The optimal number of features to include (best average validation accuracy).|\n",
    "|`best_d_one_se`\t|integer\t|The optimal number of features to include according to one-SE rule.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "z2TkU89M0Ar6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many columns, the feature selection and model fitting we are about to do can be computationally intensive. Therefore, we'll only consider the first 2000 columns. The next cell will load in that feature data into `X` and the labels into `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "hx7ejdgHqgho"
   },
   "outputs": [],
   "source": [
    "X = np.load('X.npy', allow_pickle=True)[:,:2000]\n",
    "y = np.load('y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlvk_2TcBvD1"
   },
   "source": [
    "Then, you will set aside 25% of the data for evaluating the final model at the end.  Save the result in `Xtr`, `ytr`, `Xts`, and `yts`. \n",
    "\n",
    "Use `sklearn`'s `train_test_split` with shuffling, and you will specify the `random_state = 42`so that your results will match the autograders' results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "MXeG3qa2Buci"
   },
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYxW6UM41lvF"
   },
   "source": [
    "Now, you will use 10-fold cross validation (with `sklearn`'s `KFold`, no additional shuffling since you have already shuffled the data) to evaluate model candidates as follows:\n",
    "\n",
    "* First, within each fold, compute the *absolute value* of the correlation coefficient between each column of the feature data and the target variable. (You may use `numpy`'s `corrcoef` function.) Save the results in `score_ft`, which has one entry per column per fold.\n",
    "* Then, iterate over the number of columns to include in the model - the `d` values in `d_list`. In each iteration, you will use the `d` features that had the highest absolute value of correlation coefficient in the model.\n",
    "* You will train an SVC model with a linear kernel, `C=10`, `random_state = 24`, and all other settings at their default values. You will evaluate the model on the validation data and save the accuracy score in `score_val`, which has one entry per `d` value per fold.\n",
    "\n",
    "(Note: in many cases we would standardize the data before fitting an SVC, but we won't do that here.)\n",
    "\n",
    "Write your solution in the `#grade` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "44anmfCTFowO"
   },
   "outputs": [],
   "source": [
    "d_list = np.arange(1, X.shape[1]+1) \n",
    "nd = len(d_list)\n",
    "nfold = 10\n",
    "score_ft = np.zeros((nfold, Xtr.shape[1]))\n",
    "score_val = np.zeros((nfold, nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "kf = KFold(n_splits=nfold, shuffle=False)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(Xtr)):\n",
    "    X_train, X_val = Xtr[train_index], Xtr[val_index]\n",
    "    y_train, y_val = ytr[train_index], ytr[val_index]\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if i < nfold and j < X_train.shape[1]:\n",
    "            score_ft[i, j] = np.abs(np.corrcoef(X_train[:, j], y_train)[0, 1])\n",
    "    for k, d in enumerate(d_list):\n",
    "        if i < nfold and k < nd:\n",
    "            top_features = np.argsort(score_ft[i])[-d:]\n",
    "            X_train_selected = X_train[:, top_features]\n",
    "            X_val_selected = X_val[:, top_features]\n",
    "            model = SVC(kernel='linear', C=10, random_state=24)\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            y_pred = model.predict(X_val_selected)\n",
    "            score_val[i, k] = accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `score_val` to find `best_d`, the optimal number of features to include in the model (best mean validation accuracy). (Compute the value - don't hard-code it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Q0WXXzozkwFA"
   },
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "best_d = d_list[np.argmax(np.mean(score_val, axis=1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, find `best_d_one_se`, the optimal number of features to include according to the one-SE rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "9PLIohdnI5nA"
   },
   "outputs": [],
   "source": [
    "#grade (write your code in this cell and DO NOT DELETE THIS LINE)\n",
    "mean_scores = np.mean(score_val, axis=0)\n",
    "std_scores = np.std(score_val, axis=0)\n",
    "best_score = np.max(mean_scores)\n",
    "best_score_se = std_scores[np.argmax(mean_scores)]\n",
    "one_se_threshold = best_score - best_score_se\n",
    "best_d_one_se = d_list[np.min(np.where(mean_scores >= one_se_threshold))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Workbook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
